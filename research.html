<html>
<head>
  <title>Research | Ryosuke Tanaka</title>
  <!-- main css -->
  <link rel="stylesheet" type="text/css" href="mystyle.css">
  <!-- webfonts -->
  <link href="https://fonts.googleapis.com/css?family=Rubik:400,500,700,900" rel="stylesheet">
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- favicon -->
  <link rel="icon" type="image/x-icon" href="images/favicon.svg">

  <meta name="viewport" content="width=device-width,initial-scale=1">
</head>
<body>
  <div class="container">
    <h1>Research</h1>
    <div class="navigator">
      <a href="./index.html">Home</a> > Research
    </div>
    <p>
      An important question in neuroscience is how a brain, which is composed of
      neurons operating at a fast timescale of milliseconds, manages to retain
      information and generate behaviors with longer timescales of seconds and
      minutes. My current research at <a href='https://portugueslab.com/'>Portugues lab @TUM</a>
      aims to understand neural circuit mechanisms underlying longer-timescale behaviors
      using the larval zebrafish (<i>Danio rerio</i>) as a model system.
      The larval zebrafish is one of the smallest vertebrate model organisms used
      in neuroscience with the body length of 4 mm and only about 100k neurons
      in their brain (as compared to 70M in mice and 90B in humans).
      In addition to their small size, the optical accessibility of their brains, as well
      as their genetic tractability make the zebrafish larvae amenable to various
      cutting edge experimental approaches, such as brain-wide calcium imaging,
      optogenetics, and EM-based circuit reconstructions. Currently, I am
      studying how the fish brain keeps track of its orientation in the environment
      by compining two-photon microscopy and immersrive virtual reality setups.
      I believe that obtaining a detailed, mechanistic understanding of
      fish brains is a promissing avenue towards better understanding of our own minds.
    </p>
    <p>
      During my PhD, I studied visual feature detections and their behavioral
      functions in the fruit fly <i>Drosophila</i> at <a href="https://clarklab.yale.edu/">Clark lab @Yale</a>.
      My aspiration there was to bridge "why" flies need to see certain things
      (i. e., sensory ecology / computational theory), "what" computation their
      brains performs to see what they need to see (i. e., algorithms), and "how"
      such computations are achieved by circuits of neurons (i. e., implementations),
      which together consitute canonical criteria of understanding in neuroscience.
      You can find my thesis <a href="https://www.proquest.com/openview/1ccadf07d818b36b0ebeec7f55c9fc4c">here</a>
      to check for yourself if I lived up to this lofty goal.
    </p>
    <p>
      During my undergrad and master's, I studied perception in human subjects
      at <a href='http://park.itc.u-tokyo.ac.jp/YotsumotoLab/index_en.php'>Yotsumoto lab @UTokyo</a>,
      where I was first initiated into research.
    </p>
    <p>
      You can find my Google Scholar profile <a href="https://scholar.google.com/citations?user=1-OkubgAAAAJ">here</a>.
    </p>
    <h2>Publications</h2>
    <h3>Fish works</h3>
    <ul class="publist">
      <li class="pub">Comming Soon</li>
      <li class="pubcomm">Hopefully!</li>
    </ul>
    <h3>Fly works</h3>
    <ul class="publist">

      <li class="pub">Tanaka, Zhou, Agrochao, Badwan, Au, Matos, & Clark (2023)
        Neural mechanisms to incorporate visual counterevidence in self-movement estimation. <i>Curr. Biol.</i>.
      <a href="https://www.sciencedirect.com/science/article/pii/S0960982223013696">[link]</a>
      <a href="https://www.biorxiv.org/content/10.1101/2023.01.04.522814v2">[preprint]</a></li>

      <li class="pubcomm">
        Imagine you are sitting in a train stopped at a station. There is also
        another train stopped on the opposite track. As the other train slowly
        starts to move, for a split second, you confuse their movement for your own.
        As this (hopefully familiar) example illustrates, movements of yourself
        and other objects can sometimes result in similar visual consequences.
        So, how can the brain disambiguate these scenarios? This study demonstrated
        that the fruit flies interpret wide-field visual motion not as the consequence
        of their own movements (or rotation to be preceise), when there is any
        stationary visual patterns in the view. This makes a logical sense:
        when an observer rotates, everything in the view must move in synchrony.
        By contraposition, anything appearing to be not moving suggest that the
        observer is not rotating. We also explored how detection of stationary
        patterns is implemented in the visual system of the flies.
      </li>

      <li class="pub">Mano, Choi, Tanaka, Creamer, Matos, Shomar, Badwan, Clandinin, & Clark (2023)
        Long timescale anti-directional rotation in <i>Drosophila</i> optomotor behavior. <i>eLife</i>.
      <a href="https://elifesciences.org/articles/86076">[link]</a></li>

      <li class="pubcomm">
        When presented with wide-field visual motion rotating about them,
        animals ranging from flies to humans just cannot help following it with
        their eyes or with the whole body, a reflex called the optomotor response.
        This behavior is believed to stabilize the gaze and posture, and its
        characteristics and mechansims have been under intensive study in various
        species for almost a century. But sometimes, they do just the opposite!
        This study identified when and how this paradoxical "anti-optomotor" response
        happens in the fruit flies. I contributed a part of the calcium imaging data
        for this study.
      </li>

      <li class="pub">Tanaka & Clark (2022) Neural mechanisms to exploit positional geometry for collision avoidance. <i>Curr. Biol.</i>
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0960982222005929">[link]</a>
      <a href="https://www.biorxiv.org/content/10.1101/2021.12.11.472218v1">[preprint]</a></li>

      <li class="pubcomm">
        Imagine you are driving towards an intersection without a traffic signal,
        and you see another car coming from the side, about to reach the intersection.
        If you are a safe driver, you would slow and let them pass (I hope you do).
        Actually, it has been known that flies do exactly the same thing: when they
        see an object which is about to pass in front of them (which is destined
        to appear to be moving in the back-to-front direction from their perspective),
        they slow or stop walking. In this study, we identified a specific neuron
        type called LPLC1 is necessary and sufficient for this direction-selective
        slowing behavior. Using techniques such as neurotransmitter imaging and
        connectomic analyses, we deomsntrated that LPLC1 achieves its selectivity for
        small objects moving back-to-front by combining outputs of direction-selective
        and size-selective neurons. We also uncovered a downstream pathway that
        signals potential collisions to the central brain.
      </li>

      <li class="pub">Tanaka & Clark (2022) Identifying inputs to visual projection neurons in <i>Drosophila</i> lobula by analyzing connectomic data. <i>eNeuro</i>.
      <a href="https://www.eneuro.org/content/9/2/ENEURO.0053-22.2022">[link]</a>
      <a href="https://www.biorxiv.org/content/10.1101/2022.02.02.478876v1">[preprint]</a></li>

      <li class="pubcomm">
        The release of the "hemibrain" connectome by Janelia Reseach Campus in
        January 2020 was a big game-changer in the field of fly neuroscience.
        However, unfortunately for vision researchers like myself, the hemibrain
        connectome did not include early visual neuropils, and thus input neurons
        to the lobula (which was the only visual neuropil almost fully included
        in the dataset) were fragmented and unlabeled. This paper summarizes my
        effort during the COVID lockdown to categorize these neruon fragments
        into cell types with a hiearchical clustering approach based on connectivity
        and synapse distributions.
      </li>

      <li class="pub">Tanaka & Clark (2020) Object-Displacement-Sensitive Visual Neurons Drive Freezing in <i>Drosophila</i>. <i>Curr. Biol.</i>
      <a href="https://www.sciencedirect.com/science/article/pii/S0960982220305844">[link]</a></li>

      <li class="pubcomm">
        The visual system of <i>Drosophila</i> is equipped with a suite of visual
        projection neurons (VPNs) that are supposedly tuned to behaviorally relevant
        entities (e. g. conspecifics, predators), whose activity can drive specific
        behavioral programs. This paper focused on one of these VPNs, called LC11.
        As of 2016, LC11 was known to have an exquisite tuning to small objects,
        but their behavioral functions and circuit mechanisms remained unknown.
        Here, we first showed that LC11 to be necessary for a short-timescale freezing
        behavior in flies, caused by moving small dots. We then constrained the
        circuit mechanism by which LC11 detects objects, by combining connectomic
        analyses as well as neurotransmitter imaging. Based on these data, we
        built a computational model that captures LC11's visual tuning well.
      </li>

      <li class="pub">Creamer, Mano, Tanaka, & Clark (2019) A flexible geometry for panoramic visual and optogenetic stimulation during behavior and physiology. <i>Journal of Neuroscience Methods</i>.
      <a href="https://www.sciencedirect.com/science/article/pii/S0165027019301451">[link]</a></li>

      <li class="pubcomm">
        A good visual stimulation setup is a prerequisite for a good study of
        visual behaviors. The standard solution among the fly researchers
        has been to either use custom-build LED arrays, or to simply repurpose a PC monitor.
        This paper proposed a simple, low-cost solution to create a immersive
        virtual reality setup by combining a small optical projector with a
        couple of planar mirrors. The small footprint of this geometry allowed
        us to highly parallelize behavioral setups, which enabled me to efficiently
        compare various viusal stimuli as well as flies with various different
        genotypes. I performed a proof-of-principle experiment to show that one can perform
        optogenetic stimulations with the light from the projectors simultaneously
        with visual stimuli on this setup.
      </li>

    </ul>
    <h3>Human works</h3>
    <ul class="publist">
      <li class="pub">Tanaka & Yotsumoto (2017) Passage of Time Judgments Is Relative to Temporal Expectation. <i>Front. Psych.</i>
      <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00187/full">[link]</a></li>

      <li class="pubcomm">
        There has been a growing interest among psychologists in the phenomenon
        of "passage of time judgements (POTJ)", which refers to our sense of how
        fast or slow time seems to be passing, as often expressed in the statements like
        "time flies when you are having fun". While some psychologists have drawn a strong
        distinction between POTJ and memory of duration (alluding to the distinctions
        between time-consciousness and objective time by phenomenologists), in this paper,
        I tried to argue that POTJ is basically made based on deviations between remembered
        and expected durations with several simple experiments.
      </li>

      <li class="pub">Tanaka & Yotsumoto (2016) Networks extending across dorsal and ventral visual pathways correlate with trajectory perception. <i>J. Vis.</i>
      <a href="http://jov.arvojournals.org/article.aspx?articleid=2520069">[link]</a></li>

      <li class="pubcomm">
        This paper examined neural activities of human subjects while they observed a
        visual illusion called "wriggling motion trajecotry illusion" with an fMRI.
      </li>
    </ul>

    <h3>Fly AND Human works (!)</h3>
    <ul class="publist">
      <li class="pub">Agrochao*, Tanaka*, Salazar-Gatzimas & Clark (2020) Mechanism for analogous illusory motion perception in flies and humans. <i>PNAS.</i> (*: equal contributions)
      See also <a href="https://www.wired.com/story/what-virtual-reality-for-flies-teaches-us-about-human-vision/">a WIRED coverage</a> of this paper.
      <a href="https://www.pnas.org/content/early/2020/08/19/2002937117.short">[link]</a>
      </li>

      <li class="pubcomm">
        Peripheral drift illusions, where repeated patterns of luminance gradients
        give the illusory sense of movement, is one of the most striking kinds of
        visual illusions. While many theories have been proposed to explain this illusion,
        definitive, mechanistic explanations have been lacking.
        In this study, we found that flies also percieve motion in repeated gradation
        patterns just like ourseleves, and discovered that the illusion ultimately
        originates from the imbalanced contributions of bright and dark edges for motion
        detection. Our results demonstrate that this popular illusion is a manifestation
        of a strategy to efficiently detect motion by exploiting the asymmetric
        distribution of light and dark in our visual environment, convergently
        evolved in vertebrates and insects.
        For this study, I performed human psychophysics experiments to show that
        the mechanistic model of the illusion derived from the experiments in flies
        approximately applies to humans as well, among other things.
      </li>

    </ul>

  </div>
  <footer class="footer">
  </footer>

  <!-- Script to collapse paper comments -->
  <script type="text/javascript" src="collapse-pub-comm.js"></script>
</body>
</html>
