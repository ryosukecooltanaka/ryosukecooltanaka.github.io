<html>
<head>
  <title>Research | Ryosuke Tanaka's Website</title>
  <!-- main css -->
  <link rel="stylesheet" type="text/css" href="mystyle.css">
  <!-- webfonts -->
  <link href="https://fonts.googleapis.com/css?family=Rubik:400,500,700,900" rel="stylesheet">
  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- favicon -->
  <link rel="icon" type="image/x-icon" href="images/favicon.svg">

  <meta name="viewport" content="width=device-width,initial-scale=1">
</head>
<body>
  <div class="container">
    <h1>Research</h1>
    <a href="./index.html">Home</a> > Research
    <p>
      My current research at <a href='https://portugueslab.com/'>Portugues lab @TUM</a>
      focuses on understanding the circuit mechanisms of cognitive phenomena using
      the larval zebrafish as a model system.
      The larval zebrafish is one of the smallest vertebrate model organisms used
      in neuroscience. Their small size, optical transparency, and genetic tractability
      make them amenable to diverse cutting-edge experimental approaches such as
      whole-brain calcium imaging with light-sheet microscopy, genetic manipulation
      of specific circuit elements, electronmicrograph-based circuit reconstruction,
      as well as behavioral experiments in immersive virtual reality settings.
      While the brain of these baby fish is tiny, it has a direct homology to our
      brains. I believe that obtaining the detailed, mechanistic understanding of
      the fish brian is a promissing avenue for the better understanding of our own minds.
    </p>
    <p>
      Prior to arriving at Germany, I conducted my PhD research at <a href="https://clarklab.yale.edu/">Clark lab @Yale</a>.
      My PhD research focused on understanding visual processing in fruit fly <i>Drosophila</i>,
      with a special emphasis on bridging sensory ecology to circuit mechanisms.
      Specifically, I studied how flies detect visual motion and objects (e. g. conspecifics)
      by combining behavioral experiments, two-photon imaging, optogenetics, computational modeling,
      and connectomic analyses.
      Before starting my PhD, I was engaged in psychophysical and neuroimaging
      studies of human subjects at <a href='http://park.itc.u-tokyo.ac.jp/YotsumotoLab/index_en.php'>Yotsumoto lab @UTokyo</a>,
      covering topics such as visual illusions and perception of time.
    </p>
    <p>
      You can find my Google Scholar profile <a href="https://scholar.google.com/citations?user=tRjjoXQAAAAJ&hl=en">here</a>.
    </p>
    <h2>Publications</h2>
    <h3>Fish works</h3>
    <ul class="publist">
      <li class="pub">Comming Soon</li>
      <li class="pubcomm">Hopefully!</li>
    </ul>
    <h3>Fly works</h3>
    <ul class="publist">
      <li class="pub">Tanaka & Clark (2022) Neural mechanisms to exploit positional geometry for collision avoidance. <i>Curr. Biol.</i>
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0960982222005929">[link]</a></li>

      <li class="pubcomm">
        In this paper, we studied how walking flies avoid collision with objects
        like other flies. There is a geometrical rule that any agent crossing in
        front of you (e. g. a car merging into your lane) appear to be moving in
        the back-to-front direction on your retina. We found that a specific visual
        neuron type in the fly brain called LPLC1 detects these kinds of objects
        by combining output of motion and object detectors, and its activity causes
        the fly to stop, such that it does not collide.
      </li>

      <li class="pub">Tanaka & Clark (2022) Identifying inputs to visual projection neurons in <i>Drosophila</i> lobula by analyzing connectomic data. <i>eNeuro</i>.
      <a href="https://www.eneuro.org/content/9/2/ENEURO.0053-22.2022">[link]</a></li>

      <li class="pubcomm">
        The release of the "hemibrain" connectome by Janelia Reseach Campus in
        January 2020 was a big game-changer in the field of fly neuroscience.
        However, unfortunately for vision researchers like myself, the hemibrain
        connectome did not include early visual neuropils, and thus input neurons
        to the lobula (which was the only visual neuropil almost fully included
        in the dataset) were fragmented and unlabeled. This paper summarizes my
        effort to categorize these neruon fragments into cell types with a hiearchical
        clustering approach based on connectivity and neuronal morphology.
      </li>

      <li class="pub">Tanaka & Clark (2020) Object-Displacement-Sensitive Visual Neurons Drive Freezing in <i>Drosophila</i>. <i>Curr. Biol.</i>
      <a href="https://www.sciencedirect.com/science/article/pii/S0960982220305844">[link]</a></li>

      <li class="pubcomm">
        The visual system of <i>Drosophila</i> is equipped with a suite of visual
        projection neurons (VPNs) that are supposedly tuned to behaviorally relevant visual
        features (e. g. conspecifics, predators), whose activity can drive specific
        behavioral programs. This paper focused on one of these VPNs called LC11, whose
        functions and mechanisms had been unknown at the time. Here, with a psychophysics
        experiment, I found LC11 to be necessary for a short timescale freezing
        behavior in flies caused by small moving objects.
        In addition, I constrained the mechanism of object detection in LC11 with
        connectomic analyses as well as direct visualization of their neurotransmitter
        inputs, and built a computational model that can explain their visual tuning well.
      </li>

      <li class="pub">Creamer, Mano, Tanaka, & Clark (2019) A flexible geometry for panoramic visual and optogenetic stimulation during behavior and physiology. <i>Journal of Neuroscience Methods</i>.
      <a href="https://www.sciencedirect.com/science/article/pii/S0165027019301451">[link]</a></li>

      <li class="pubcomm">
        It is crucial to have a good visual stimulation setup to study the neural
        bases of visual behaviors. The standard solution among the fly researchers
        has been to either use custom-build LED arrays, or to simply repurpose a PC monitor.
        This paper proposed a low-cost, low-footprint solution to create a immersive
        virtual reality visual stimulation setup with a small DLP projector and a
        couple of mirrors. The highly parallelized psychophysics rigs using this
        geometry were crucial to the succsess of my other experimental projects.
        I performed a proof-of-principle experiment to show that one can perform
        optogenetic stimulations with the light from the projectors simultaneously
        with visual stimuli on this setup.
      </li>

    </ul>
    <h3>Human works</h3>
    <ul class="publist">
      <li class="pub">Tanaka & Yotsumoto (2017) Passage of Time Judgments Is Relative to Temporal Expectation. <i>Front. Psych.</i>
      <a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00187/full">[link]</a></li>

      <li class="pubcomm">
        There has been a growing interest among psychologists in the phenomenon
        of "passage of time judgements (POTJ)", which refers to our sense of how
        fast or slow time is progressing, as often expressed in the statements like
        "time flies when you are having fun". While some psychologists have drawn a strong
        distinction between POTJ and memory of duration (alluding to the distinctions
        of time-consciousness and objective time by phenomenologists), in this paper,
        I tried to argue that POTJ is basically made based on deviations between remembered
        and expected durations with several simple experiments.
      </li>

      <li class="pub">Tanaka & Yotsumoto (2016) Networks extending across dorsal and ventral visual pathways correlate with trajectory perception. <i>J. Vis.</i>
      <a href="http://jov.arvojournals.org/article.aspx?articleid=2520069">[link]</a></li>

      <li class="pubcomm">
        This paper examined neural activities of human subjects while observing a
        visual illusion where dots moving straight elicit illusory sense of curvature
        with an fMRI, and marks the beginning of my publishing career.
      </li>
    </ul>

    <h3>Fly AND Human works (!)</h3>
    <ul class="publist">
      <li class="pub">Agrochao*, Tanaka*, Salazar-Gatzimas & Clark (2020) Mechanism for analogous illusory motion perception in flies and humans. <i>PNAS.</i> (*: equal contributions)
      See also <a href="https://www.wired.com/story/what-virtual-reality-for-flies-teaches-us-about-human-vision/">a WIRED coverage</a> of this paper.
      <a href="https://www.pnas.org/content/early/2020/08/19/2002937117.short">[link]</a>
      </li>

      <li class="pubcomm">
        Peripheral drift illusions, where repeated patterns of luminance gradients
        give the illusory sense of movement, is one of the most striking kinds of
        visual illusions. While many theories to explain the illusion have been
        proposed, definitive, mechanistic explanations have been lacking.
        In this study, we found that flies also percieve motion in repeated gradation
        patterns just like ourseleves, and discovered that the illusion ultimately
        originates from the imbalanced contributions of bright and dark edges for motion
        detection. Our results demonstrate that this popular illusion is a manifestation
        of a strategy to efficiently detect motion by exploiting the asymmetric
        distribution of light and dark in our visual environment, convergently
        evolved in vertebrates and insects.
        For this study, most notably, I performed human psychophysics experiments to show that
        the mechanistic model of the illusion derived from the experiments in flies
        approximately applies to humans as well, among others.
      </li>

    </ul>

  </div>
  <footer class="footer">
  </footer>

  <!-- Script to collapse paper comments -->
  <script type="text/javascript" src="collapse-pub-comm.js"></script>
</body>
</html>
